{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第13章 无优化问题和神经网络优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.1 引言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的核心是是神经元之间的连接权重，确定权重的过程称为训练或学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络训练方法为反向传播算法，该算法基于无约束的优化问题，并利用地图算法进行求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个神经元表示一个映射，通常是多输入单输出。神经元的输出是输入之和的函数，该函数通常称为激活函数。某个神经元的输出可以用作多个其他神经元的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络包括多个相互连接的神经元，各神经元的输入为其他神经元的输出的加权。在前馈神经网络中，神经元按照不同的层次进行连接。每个神经元只接受来自上一层次神经元的输出，是上一层次神经元输出的加权。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络的第一层称为输入层，最后一层称为输出层，输入层和输出层之间为中间层（隐藏层）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个映射$\\mathbf{F}:\\mathbb{R}^n\\to\\mathbb{R}^m$，可以采用特定结构的神经网络实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确定数据对$\\left(\\mathbf{x}_{d,1},\\mathbf{y}_{d,1}\\right),\\left(\\mathbf{x}_{d,1},\\mathbf{y}_{d,1}\\right),\\dots,\\left(\\mathbf{x}_{d,p},\\mathbf{y}_{d,p}\\right)\\in\\mathbb{R}^n\\times\\mathbb{R}^m$，其中，$\\mathbf{y}_{d,i}$为映射$\\mathbf{F}$的输出，对应输入为$\\mathbf{x}_{d,i}$，即$\\mathbf{y}_{d,i}=\\mathbf{F}\\left(\\mathbf{x}_{d,i}\\right)$，以数据对$\\{\\left(\\mathbf{x}_{d,1},\\mathbf{y}_{d,1}\\right),\\left(\\mathbf{x}_{d,1},\\mathbf{y}_{d,1}\\right),\\dots,\\left(\\mathbf{x}_{d,p},\\mathbf{y}_{d,p}\\right)\\}$作为训练集对神经网络进行训练。利用训练算法，以网络实际输出和制定输出之间的误差，即$\\mathbf{y}_{d,i}=\\mathbf{F}\\left(\\mathbf{y}_{d,i}\\right)$和神经网络在输入$\\mathbf{x}_{d,i}$下的输出之间的差值为依据，对连接权重进行调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络训练问题可归纳为优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.2 单个神经元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经元可以由如下函数给出：\n",
    "$$y=f_a\\left(\\sum_{i=1}^{n}w_i x_i\\right)=f_a\\left(\\mathbf{x}^\\top\\mathbf{w}\\right)$$\n",
    "其中，$\\mathbf{x}=\\left[x_1,x_2,\\dots,x_n\\right]^\\top\\in\\mathbb{R}^n$表示输入向量，$y\\in\\mathbb{R}$为输出，$\\mathbf{w}=\\left[w_1,w_2,\\dots,w_n\\right]^\\top\\in\\mathbb{R}^n$为权重向量，$f_a$为任意可微激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定映射$\\mathbf{F}:\\mathbb{R}^n\\to\\mathbb{R}$，希望通过训练$w_1,w_2,\\dots,w_n$，使得该神经元能够尽可能地逼近$\\mathbf{F}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.3 反向传播算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设神经网络共有$n$个输入$x_i,i=1,2,\\dots,n$；$m$个输出$y_s,s=1,2,\\dots,m$。中间层包括$l$个神经元，中间层神经元的输出为$z_j,j=1,2,\\dots,l$。中间层神经元的激活函数为$f_j^h,j=1,2,\\dots,l$，输出层神经元的激活函数为$f_s^o,s=1,2,\\dots,m$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "令$w_{ji}^h,i=1,2,\\dots,n;j=1,2,\\dots,l$表示中间层输入对应的权重，$w_{sj}^o,j=1,2,\\dots,l;s=1,2,\\dots,m$表示输出层输入对应的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令$v_j$表示中间层第$j$个神经元的输入，有$v_j=\\sum_{i=1}^n w_{ji}^h x_i$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令$z_j$表示中间层第$j$个神经元的输出，有$z_j=f_j^h\\left(v_j\\right)=f_j^h\\left(\\sum_{i=1}^n w_{ji}^h x_i\\right)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出层第$s$个神经元的输出为$y_s=f_s^o\\left(\\sum_{j=1}^lw_{sj}^oz_j\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入$x_i,i=1,2,\\dots,n$和第$s$个输出$y_s$之间的关系为\n",
    "$$y_s=f_s^o\\left(\\sum_{j=1}^lw_{sj}^oz_j\\right) \\\\\n",
    "=f_s^o\\left(\\sum_{j=1}^lw_{sj}^o f_j^h\\left(v_j\\right)\\right) \\\\\n",
    "=f_s^o\\left(\\sum_{j=1}^lw_{sj}^o f_j^h\\left(\\sum_{i=1}^n w_{ji}^h x_i\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "假定训练数据对$\\left(\\mathbf{x}_d,\\mathbf{y}_d\\right)$，$\\mathbf{x}_d=\\left[x_{d1},x_{d2},\\dots,x_{dn}\\right]^\\top\\in\\mathbb{R}^n$，$\\mathbf{y}_d\\in\\mathbb{R}^m$。神经网络的训练指的是调整网络的连接权重，使得在给定的输入$\\mathbf{x}_d$下，输出能够尽可能地接近于$\\mathbf{y}_d$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "形式上，可以得到如下优化问题：\n",
    "$$\\min\\frac{1}{2}\\sum_{s=1}^m\\left(y_{ds}-y_s\\right)^2$$\n",
    "其中，$y_s,s=1,2,\\dots,m$表示神经网络在输入$x_{d1},x_{d2},\\dots,x_{dn}$下的实际输出：\n",
    "$$y_s=f_s^o\\left(\\sum_{j=1}^lw_{sj}^of_j^h\\left(\\sum_{i=1}^nw_{ji}^hx_i\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策变量为所有权重，即$w_{ji}^h,w_{sj}^o,i=1,2,\\dots,n;j=1,2,\\dots,l;s=1,2,\\dots,m$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策变量向量形式为$\\mathbf{w}=\\{w_{ji}^h,w_{sj}^o:i=1,2,\\dots,n,j=1,2,\\dots,l,s=1,2,\\dots,m\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标函数\n",
    "$$\\mathbf{E}\\left(\\mathbf{w}\\right)=\\frac{1}{2}\\sum_{s=1}^m\\left(y_{ds}-y_s\\right)^2 \\\\\n",
    "=\\frac{1}{2}\\sum_{s=1}^m\\left(y_{ds}-f_s^o\\left(\\sum_{j=1}^lw_{sj}^of_j^h\\left(\\sum_{i=1}^nw_{ji}^hx_i\\right)\\right)\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可利用固定步长梯度法来求解。需要计算目标函数$\\mathbf{E}$关于$\\mathbf{w}$中每个元素的偏导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算函数$\\mathbf{E}$关于$w_{sj}^o$的偏导数。固定$i$、$j$、$s$，将函数$\\mathbf{E}$改写为\n",
    "$$\\mathbf{E}\\left(\\mathbf{w}\\right)=\\frac{1}{2}\\sum_{p=1}^m\\left(y_{dp}-f_p^o\\left(\\sum_{q=1}^lw_{pq}^oz_q\\right)\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$q=1,2,\\dots,l$，有\n",
    "$$z_q=f_q^h\\left(\\sum_{i=1}^nw_{qi}^hx_{di}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "利用链式法则，可得\n",
    "$$\\frac{\\partial\\mathbf{E}}{\\partial w_{sj}^o}\\left(\\mathbf{w}\\right)=-\\left(y_{ds}-y_s\\right)f_s^{o'}\\left(\\sum_{q=1}^lw_{sq}^oz_q\\right)z_j$$\n",
    "其中，$f_s^{o'}:\\mathbb{R}\\to\\mathbb{R}$表示函数$f_s^{o}$的导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为简化描述，定义\n",
    "$$\\delta_s=\\left(y_{ds}-y_s\\right)f_s^{o'}\\left(\\sum_{q=1}^lw_{sq}^oz_q\\right)z_j$$\n",
    "$\\delta_s$是输出误差（神经网络的实际输出$y_s$与要求输出$y_ds$之间的差值）进行缩放的结果，缩放因子为$f_s^{o'}\\left(\\sum_{q=1}^lw_{sq}^oz_q\\right)z_j$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用$\\delta_s$的表达式，可得\n",
    "$$\\frac{\\partial\\mathbf{E}}{\\partial w_{sj}^o}\\left(\\mathbf{w}\\right)=-\\delta_sz_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算函数$\\mathbf{E}$关于$w_{ji}^h$的偏导数。固定$i$、$j$、$s$，将函数$\\mathbf{E}$改写为\n",
    "$$\\mathbf{E}\\left(\\mathbf{w}\\right)=\\frac{1}{2}\\sum_{p=1}^m\\left(y_{ds}-f_s^o\\left(\\sum_{q=1}^lw_{pq}^of_q^h\\left(\\sum_{r=1}^nw_{qr}^hx_{dr}\\right)\\right)\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用链式法则，可得\n",
    "$$\\frac{\\partial\\mathbf{E}}{\\partial w_{ji}^h}\\left(\\mathbf{w}\\right)=-\\sum_{p=1}^m\\left(y_{dp}-y_p\\right)f_p^{o'}\\left(\\sum_{q=1}^lw_{pq}^oz_q\\right)w_{pj}^of_j^{h'}\\left(\\sum_{r=1}^nw_{jr}^hx_{dr}\\right)x_{di}$$\n",
    "其中，$f_j^{h'}:\\mathbb{R}\\to\\mathbb{R}$表示函数$f_j^{h}$的导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用$\\delta_s$的表达式，可得\n",
    "$$\\frac{\\partial\\mathbf{E}}{\\partial w_{ji}^h}\\left(\\mathbf{w}\\right)=-\\left(\\sum_{p=1}^m\\delta_pw_{pj}^o\\right)f_j^{h'}\\left(v_j\\right)x_{di}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "权重$w_{sj}^o$和$w_{ji}^h$的迭代更新公式:\n",
    "$$w_{sj}^{o\\left(k+1\\right)}=w_{sj}^{o\\left(k\\right)}+\\eta\\delta_s^{\\left(k\\right)}z_j^{\\left(k\\right)} \\\\\n",
    "w_{ji}^{h\\left(k+1\\right)}=w_{ji}^{h\\left(k\\right)}+\\eta\\left(\\sum_{p=1}^m\\delta_p^{\\left(k\\right)}w_{pj}^{o\\left(k\\right)}\\right)f_j^{h'}\\left(v_j^{\\left(k\\right)}\\right)x_{di}$$\n",
    "其中，\n",
    "$$v_j^{\\left(k\\right)}=\\sum_{i=1}^nw_{ji}^{h\\left(k\\right)}x_{di} \\\\\n",
    "z_j^{\\left(k\\right)}=f_j^k\\left(v_j^{\\left(k\\right)}\\right)  \\\\\n",
    "y_s^{\\left(k\\right)}=f_s^o\\left(\\sum_{q=1}^lw_{sq}^{o\\left(k\\right)}z_q^{\\left(k\\right)}\\right) \\\\\n",
    "\\delta_s^{\\left(k\\right)}=\\left(y_{ds}-y_s^{\\left(k\\right)}\\right)f_s^{o'}\\left(\\sum_{q=1}^lw_{sq}^{o\\left(k\\right)}z_q^{\\left(k\\right)}\\right)$$\n",
    "$\\eta$表示为固定步长。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
